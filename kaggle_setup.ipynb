{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Aether - Kaggle Notebook Setup\n",
    "\n",
    "This notebook sets up and runs Project Aether on Kaggle with GPU support.\n",
    "\n",
    "## Features\n",
    "- Automatic GPU detection and setup\n",
    "- Model downloads (Stable Diffusion 1.4 - less censored for research)\n",
    "- All three phases: Probe Training, PPO Training, Evaluation\n",
    "- **Empirical layer sensitivity measurement** (FID & SSR) for optimal intervention points\n",
    "- Optimized for Kaggle's P100 GPU (16GB VRAM)\n",
    "- **Nudity-focused** content filtering for clearer concept boundaries\n",
    "\n",
    "## Important Notes\n",
    "- **Model:** Uses `CompVis/stable-diffusion-v1-4` (less censored than SD 1.5)\n",
    "- **Focus:** Nudity-only content (not gore/violence) for better probe training\n",
    "- **Filtering:** Strict thresholds (≥50% nudity, ≥60% inappropriate, hard prompts only)\n",
    "- **Kaggle:** 30-hour session limit, 9-hour GPU limit per session\n",
    "\n",
    "## References\n",
    "- **FID Metric:** Heusel et al. (2017). \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.\" NeurIPS 2017.\n",
    "- **Linear Probing:** Alain & Bengio (2016). \"Understanding Intermediate Layers Using Linear Classifier Probes.\" arXiv:1610.01644.\n",
    "- **PPO:** Schulman et al. (2017). \"Proximal Policy Optimization Algorithms.\" arXiv:1707.06347."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA (Kaggle uses CUDA 11.8 or 12.1)\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install diffusers transformers accelerate safetensors\n",
    "!pip install gymnasium numpy scikit-learn matplotlib tqdm\n",
    "!pip install pyyaml pillow lpips\n",
    "!pip install datasets  # For I2P dataset\n",
    "!pip install pytorch-fid  # For FID metric (Heusel et al., 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub\n",
    "!git clone https://github.com/Anastasia-Deniz/project-aether.git\n",
    "%cd project-aether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Verify GPU\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Create necessary directories\n",
    "!mkdir -p data/latents checkpoints/probes outputs/ppo outputs/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Phase 1 - Collect Latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect latents for probe training\n",
    "# Kaggle P100 has 16GB VRAM, similar to Colab T4\n",
    "# Using SD 1.4 (20 steps) - less censored than SD 1.5, better for research\n",
    "# Focus on nudity only with strict quality thresholds\n",
    "!python scripts/collect_latents.py \\\n",
    "    --num_samples 100 \\\n",
    "    --num_steps 20 \\\n",
    "    --device cuda \\\n",
    "    --model_id CompVis/stable-diffusion-v1-4 \\\n",
    "    --focus_nudity \\\n",
    "    --hard_only \\\n",
    "    --min_inappropriate_pct 60.0 \\\n",
    "    --min_nudity_pct 50.0 \\\n",
    "    --save_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Phase 1 - Train Probes\n",
    "\n",
    "Train linear probes at each timestep to measure concept separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear probes\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=os.path.getmtime)\n",
    "if latents_dirs:\n",
    "    latest_latents = latents_dirs[-1]\n",
    "    print(f\"Using latents from: {latest_latents}\")\n",
    "    \n",
    "    # Option A: Train with improved heuristics (faster, default)\n",
    "    !python scripts/train_probes.py --latents_dir {latest_latents}\n",
    "    \n",
    "    # Option B: Train with empirical measurements (better accuracy, requires Step 5.5)\n",
    "    # !python scripts/train_probes.py --latents_dir {latest_latents} --use_empirical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5: (Optional) Measure Empirical Layer Sensitivity ⭐ NEW\n",
    "\n",
    "**Recommended for best results:** Measure FID and SSR empirically instead of using heuristics.\n",
    "\n",
    "This step runs small steering experiments to measure:\n",
    "- **Quality preservation**: FID between steered and unsteered images (Heusel et al., 2017)\n",
    "- **Steering effectiveness**: SSR improvement from steering\n",
    "\n",
    "**Note:** This takes additional time (~30-60 min) but provides more accurate sensitivity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure empirical layer sensitivity (FID and SSR)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=os.path.getmtime)\n",
    "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=os.path.getmtime)\n",
    "\n",
    "if latents_dirs:\n",
    "    latest_latents = latents_dirs[-1]\n",
    "    \n",
    "    # Use probe from Step 5 if available\n",
    "    probe_path = None\n",
    "    if probe_dirs:\n",
    "        latest_probe = probe_dirs[-1] / 'pytorch'\n",
    "        if latest_probe.exists():\n",
    "            probe_path = str(latest_probe)\n",
    "            print(f\"Using probe: {probe_path}\")\n",
    "    \n",
    "    print(f\"Measuring empirical sensitivity for: {latest_latents}\")\n",
    "    print(\"This may take 30-60 minutes...\")\n",
    "    \n",
    "    if probe_path:\n",
    "        !python scripts/measure_layer_sensitivity.py \\\n",
    "            --latents_dir {latest_latents} \\\n",
    "            --num_samples 20 \\\n",
    "            --device cuda \\\n",
    "            --probe_path {probe_path}\n",
    "    else:\n",
    "        !python scripts/measure_layer_sensitivity.py \\\n",
    "            --latents_dir {latest_latents} \\\n",
    "            --num_samples 20 \\\n",
    "            --device cuda\n",
    "    \n",
    "    print(\"\\nNow re-run Step 5 with --use_empirical flag to use these measurements!\")\n",
    "else:\n",
    "    print(\"No latents found! Run Step 4 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Phase 2 - Train PPO Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO policy with Kaggle-optimized config\n",
    "!python scripts/train_ppo.py --config configs/colab_optimized.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Phase 3 - Evaluate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained policy\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ppo_dirs = sorted(Path('outputs/ppo').glob('aether_ppo_*'), key=os.path.getmtime)\n",
    "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=os.path.getmtime)\n",
    "\n",
    "if ppo_dirs and probe_dirs:\n",
    "    latest_policy = ppo_dirs[-1] / 'final_policy.pt'\n",
    "    latest_probe = probe_dirs[-1] / 'pytorch'\n",
    "    \n",
    "    if latest_policy.exists() and latest_probe.exists():\n",
    "        print(f\"Evaluating: {latest_policy}\")\n",
    "        !python scripts/evaluate_ppo.py \\\n",
    "            --policy_path {latest_policy} \\\n",
    "            --probe_path {latest_probe} \\\n",
    "            --num_samples 50 \\\n",
    "            --device cuda\n",
    "    else:\n",
    "        print(\"Policy or probe not found!\")\n",
    "else:\n",
    "    print(\"No training runs or probes found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results to Kaggle Output\n",
    "\n",
    "Kaggle automatically saves all files in `/kaggle/working/` to the output dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy important outputs to /kaggle/working/ for automatic saving\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('/kaggle/working/project-aether-results')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy all important outputs\n",
    "if Path('outputs').exists():\n",
    "    shutil.copytree('outputs', output_dir / 'outputs', dirs_exist_ok=True)\n",
    "if Path('checkpoints').exists():\n",
    "    shutil.copytree('checkpoints', output_dir / 'checkpoints', dirs_exist_ok=True)\n",
    "if Path('data/latents').exists():\n",
    "    shutil.copytree('data/latents', output_dir / 'data/latents', dirs_exist_ok=True)\n",
    "\n",
    "print(f\"✓ Results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}