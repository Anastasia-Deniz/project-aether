# Project Aether - PPO Training Configuration (IMPROVED)
# Phase 2: Extended training with improved hyperparameters
# Based on evaluation results - addressing low SSR and high FPR

# Compute settings
compute:
  device: "cuda"
  dtype: "float16"  # CRITICAL: Half precision saves ~50% VRAM

# Environment settings
env:
  # Using LCM (Latent Consistency Model) for faster inference
  model_id: "CompVis/stable-diffusion-v1-4"
  num_inference_steps: 20
  guidance_scale: 7.5
  
  # Low-rank steering dimension
  steering_dim: 256
  
  # Action constraints
  max_action_norm: 0.1
  
  # Intervention window (from sensitivity analysis)
  intervention_start: 5
  intervention_end: 15
  
  # Latent encoder for reduced observation space
  use_latent_encoder: true
  encoded_latent_dim: 256

# Policy network (memory optimized)
policy:
  hidden_dims: [256, 128]  # Memory optimized for 6GB VRAM
  activation: "relu"
  latent_encoder_dim: 256

# PPO hyperparameters (IMPROVED for better learning)
ppo:
  learning_rate: 2.0e-4  # Slightly reduced for more stable learning
  
  # Rollout steps (keeping memory optimized)
  n_steps: 64  # Memory constraint - keep at 64
  
  # Batch size (keeping memory optimized)
  batch_size: 8  # Memory constraint - keep at 8
  
  n_epochs: 6  # Increased from 4 for more learning per rollout
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  max_grad_norm: 0.5
  
  # RECOMMENDATION #1: Significantly increased training time
  total_timesteps: 150000  # Increased from 50,000 to 150,000 (3x more training)

# Reward settings (IMPROVED)
reward:
  # RECOMMENDATION #2: Tuned lambda to balance safety and quality
  # Lower lambda = more aggressive steering (better SSR, but may increase FPR)
  # Higher lambda = more conservative steering (lower FPR, but may reduce SSR)
  # Current: 0.5 -> Try 0.3 for more aggressive safety steering
  lambda_transport: 0.3  # Reduced from 0.5 to prioritize safety over transport cost
  safety_classifier: null
  # Path to trained probes from Phase 1
  probe_path: "./checkpoints/probes/run_20251213_125128/pytorch/"

# Training settings
training:
  experiment_name: "aether_ppo_improved"
  
  # More frequent logging due to smaller rollouts
  log_interval: 5
  save_interval: 10000  # More frequent checkpoints for longer training
  
  # Early stopping (relaxed for longer training)
  early_stopping:
    enabled: true
    patience: 30000  # Increased patience
    min_delta: 0.01
    metric: "rollout/ep_rew_mean"

# Weights & Biases logging
wandb:
  enabled: false
  project: "project-aether"
  entity: null
  tags:
    - "phase2"
    - "ppo"
    - "steering"
    - "improved"
    - "extended_training"



