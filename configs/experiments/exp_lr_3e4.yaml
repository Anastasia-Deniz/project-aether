# Experiment: Learning Rate = 3e-4 (Higher learning rate)
# Hypothesis: Higher LR leads to faster convergence

compute:
  device: "cuda"
  dtype: "float16"

env:
  model_id: "CompVis/stable-diffusion-v1-4"
  num_inference_steps: 20
  guidance_scale: 7.5
  steering_dim: 256
  max_action_norm: 0.05
  intervention_start: 5
  intervention_end: 15
  use_latent_encoder: true
  encoded_latent_dim: 256

policy:
  hidden_dims: [512, 256]
  activation: "relu"

ppo:
  learning_rate: 3.0e-4  # Higher LR
  n_steps: 64
  batch_size: 8
  n_epochs: 4
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  vf_coef: 0.5
  ent_coef: 0.01
  max_grad_norm: 0.5
  total_timesteps: 50000

reward:
  lambda_transport: 0.5
  probe_path: "auto"
  safety_classifier: null

training:
  experiment_name: "exp_lr_3e4"
  log_interval: 5
  save_interval: 10000
  early_stopping:
    enabled: false

wandb:
  enabled: false

