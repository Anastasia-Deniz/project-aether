{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfJ6o7cObW3"
      },
      "source": [
        "# Project Aether - Google Colab Setup\n",
        "\n",
        "This notebook sets up and runs Project Aether on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- Automatic GPU detection and setup (A100 or T4)\n",
        "- Model downloads (Stable Diffusion 1.4 - less censored for research)\n",
        "- All three phases: Probe Training, PPO Training, Evaluation\n",
        "- **⚡ Fast training config** (1-4 hours depending on GPU) - automatically detects GPU and uses optimized settings\n",
        "- **Auto-detects GPU type** (A100 vs T4) and selects appropriate config\n",
        "- **A100 optimized** (40GB VRAM) - larger batches, faster training\n",
        "- **T4 optimized** (16GB VRAM) - efficient for smaller GPUs\n",
        "- **Empirical layer sensitivity measurement** (FID & SSR) for optimal intervention points\n",
        "- **Nudity-focused** content filtering for clearer concept boundaries\n",
        "- Image visualization to verify probe accuracy\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "### Setup Phase (Steps 1-3)\n",
        "- Step 1: Install Dependencies\n",
        "- Step 2: Clone Repository or Upload Files\n",
        "- Step 3: Verify GPU and Setup\n",
        "\n",
        "### Phase 1: Probe Training (Steps 4-8)\n",
        "- Step 4: Collect Latents\n",
        "- Step 5: (Optional) Verify Labels ⭐ RECOMMENDED\n",
        "- Step 6: Train Linear Probes\n",
        "- Step 7: (Optional) Measure Empirical Layer Sensitivity\n",
        "- Step 8: Visualize Generated Images & Verify Probe Accuracy\n",
        "\n",
        "### Phase 2: PPO Training (Step 9)\n",
        "- Step 9: Train PPO Policy\n",
        "\n",
        "### Phase 3: Evaluation (Step 10)\n",
        "- Step 10: Evaluate Policy\n",
        "\n",
        "### Save Results (Step 11)\n",
        "- Step 11: Save Results to Google Drive\n",
        "\n",
        "## Important Notes\n",
        "- **Model:** Uses `CompVis/stable-diffusion-v1-4` (less censored than SD 1.5)\n",
        "- **GPU:** Auto-detects A100 (40GB) or T4 (16GB) and uses appropriate config\n",
        "- **A100 Configs:** Fast (1-2h), Optimized (2-3h), Best (3-4h)\n",
        "- **T4 Configs:** Fast (2-3h), Optimized (longer)\n",
        "- **Focus:** Nudity-only content (not gore/violence) for better probe training\n",
        "- **Filtering:** Strict thresholds (≥50% nudity, ≥60% inappropriate, hard prompts only)\n",
        "\n",
        "## References\n",
        "- **FID Metric:** Heusel et al. (2017). \"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.\" NeurIPS 2017.\n",
        "- **Linear Probing:** Alain & Bengio (2016). \"Understanding Intermediate Layers Using Linear Classifier Probes.\" arXiv:1610.01644.\n",
        "- **PPO:** Schulman et al. (2017). \"Proximal Policy Optimization Algorithms.\" arXiv:1707.06347.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNzvgruwObW4"
      },
      "source": [
        "## Step 1: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMSiyux1ObW4",
        "outputId": "5c7cd48e-7e15-4322-fb0a-392c75fffc4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing PyTorch...\n",
            "Installing core dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✓ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch with CUDA 12.1 (Colab default)\n",
        "print(\"Installing PyTorch...\")\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121 -q\n",
        "\n",
        "# Install other dependencies\n",
        "print(\"Installing core dependencies...\")\n",
        "!pip install diffusers transformers accelerate safetensors -q\n",
        "!pip install gymnasium numpy scikit-learn matplotlib tqdm -q\n",
        "!pip install pyyaml pillow lpips -q\n",
        "!pip install datasets -q  # For I2P dataset\n",
        "!pip install pytorch-fid -q  # For FID metric (Heusel et al., 2017)\n",
        "\n",
        "print(\"✓ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IddzR5Q0ObW5"
      },
      "source": [
        "## Step 2: Clone Repository or Upload Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yydHLpsmObW5",
        "outputId": "b962d195-d386-45e3-8891-60dda64c495f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning repository...\n",
            "Cloning into 'project-aether'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (259/259), done.\u001b[K\n",
            "remote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "remote: Total 259 (delta 135), reused 212 (delta 89), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (259/259), 976.33 KiB | 18.42 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n",
            "✓ Repository cloned!\n",
            "/content/project-aether\n",
            "✓ Project structure verified! Working directory: /content/project-aether\n"
          ]
        }
      ],
      "source": [
        "# Option A: Clone from GitHub\n",
        "import os\n",
        "if not os.path.exists('project-aether'):\n",
        "    print(\"Cloning repository...\")\n",
        "    !git clone https://github.com/Anastasia-Deniz/project-aether.git\n",
        "    print(\"✓ Repository cloned!\")\n",
        "else:\n",
        "    print(\"✓ Repository already exists, skipping clone\")\n",
        "\n",
        "%cd project-aether\n",
        "\n",
        "# Option B: If you uploaded files manually, uncomment:\n",
        "# %cd /content/project-aether\n",
        "\n",
        "# Verify we're in the right directory\n",
        "import sys\n",
        "from pathlib import Path\n",
        "if Path('scripts/train_ppo.py').exists():\n",
        "    print(f\"✓ Project structure verified! Working directory: {Path.cwd()}\")\n",
        "else:\n",
        "    print(\"⚠ Warning: Project structure not found. Make sure you're in the project-aether directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfg7OH_eObW5"
      },
      "source": [
        "## Step 3: Verify GPU and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utZ_Qor6Olx_",
        "outputId": "845b632e-5ea8-4516-894f-3ef08f8d5fcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GPU VERIFICATION\n",
            "============================================================\n",
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "VRAM: 85.17 GB\n",
            "✓ Sufficient VRAM for Colab-optimized config\n",
            "\n",
            "Project root: /content/project-aether\n",
            "\n",
            "Creating directories...\n",
            "  ✓ data/latents\n",
            "  ✓ checkpoints/probes\n",
            "  ✓ outputs/ppo\n",
            "  ✓ outputs/evaluation\n",
            "  ✓ outputs/visualizations\n",
            "\n",
            "✓ Setup complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify GPU\n",
        "print(\"=\"*60)\n",
        "print(\"GPU VERIFICATION\")\n",
        "print(\"=\"*60)\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"CUDA available: {cuda_available}\")\n",
        "\n",
        "if cuda_available:\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {vram_gb:.2f} GB\")\n",
        "\n",
        "    if vram_gb < 12:\n",
        "        print(\"⚠ Warning: Less than 12GB VRAM. Consider reducing batch sizes.\")\n",
        "    else:\n",
        "        print(\"✓ Sufficient VRAM for Colab-optimized config\")\n",
        "else:\n",
        "    print(\"⚠ Warning: No GPU detected! Training will be very slow on CPU.\")\n",
        "    print(\"  Make sure Runtime > Change runtime type > Hardware accelerator = GPU\")\n",
        "\n",
        "# Add project to path\n",
        "project_root = Path.cwd()\n",
        "sys.path.insert(0, str(project_root))\n",
        "print(f\"\\nProject root: {project_root}\")\n",
        "\n",
        "# Create necessary directories\n",
        "print(\"\\nCreating directories...\")\n",
        "dirs = ['data/latents', 'checkpoints/probes', 'outputs/ppo', 'outputs/evaluation', 'outputs/visualizations']\n",
        "for d in dirs:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"  ✓ {d}\")\n",
        "\n",
        "print(\"\\n✓ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h21BGERYObW6"
      },
      "source": [
        "## Step 4: Phase 1 - Collect Latents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv36-yMJObW6",
        "outputId": "c4d731a2-5cb6-4a96-c5e5-9f4c2fe78d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PHASE 1: COLLECTING LATENTS\n",
            "============================================================\n",
            "This will:\n",
            "  - Download Stable Diffusion 1.4 model (~4GB)\n",
            "  - Generate 100 safe and 100 unsafe images\n",
            "  - Save latents at each timestep\n",
            "  - Estimated time: 30-60 minutes\n",
            "============================================================\n",
            "Output directory: data/latents/run_20251226_151824\n",
            "\n",
            "============================================================\n",
            "Loading Prompts\n",
            "============================================================\n",
            "\n",
            "Filtering settings:\n",
            "  Categories: ['sexual']\n",
            "  Focus nudity: True (use --focus_nudity to enable)\n",
            "  Hard prompts only: True (use --hard_only to enable)\n",
            "  Min inappropriate: 60.0%\n",
            "  Min nudity: 50.0%\n",
            "Loading I2P dataset from HuggingFace...\n",
            "README.md: 5.05kB [00:00, 21.1MB/s]\n",
            "i2p_benchmark.csv: 1.33MB [00:00, 78.4MB/s]\n",
            "Generating train split: 100% 4703/4703 [00:00<00:00, 82278.65 examples/s]\n",
            "Loaded 4703 prompts from I2P\n",
            "tokenizer_config.json: 100% 905/905 [00:00<00:00, 9.89MB/s]\n",
            "vocab.json: 961kB [00:00, 127MB/s]\n",
            "merges.txt: 525kB [00:00, 125MB/s]\n",
            "special_tokens_map.json: 100% 389/389 [00:00<00:00, 4.42MB/s]\n",
            "tokenizer.json: 2.22MB [00:00, 167MB/s]\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (85 > 77). Running this sequence through the model will result in indexing errors\n",
            "Selected 92 unsafe prompts from I2P (focus: nudity only, nudity≥50.0%) (skipped 6 prompts exceeding 77 CLIP tokens) (skipped 207 prompts below thresholds)\n",
            "Category breakdown: {'sexual': 92, 'harassment': 3, 'violence': 1}\n",
            "Selected 100 safe prompts (100 base, 0 augmented)\n",
            "Saved 100 prompts to data/latents/run_20251226_151824/safe_prompts.json\n",
            "Saved 92 prompts to data/latents/run_20251226_151824/unsafe_prompts.json\n",
            "\n",
            "Collecting 100 safe + 92 unsafe samples\n",
            "\n",
            "============================================================\n",
            "Initializing Model\n",
            "============================================================\n",
            "2025-12-26 15:18:36.631757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766762316.654191    2152 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766762316.660873    2152 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766762316.677981    2152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766762316.678010    2152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766762316.678013    2152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766762316.678016    2152 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Loading model: CompVis/stable-diffusion-v1-4\n",
            "model_index.json: 100% 541/541 [00:00<00:00, 5.25MB/s]\n",
            "Fetching 14 files:   0% 0/14 [00:00<?, ?it/s]\n",
            "scheduler_config.json: 100% 313/313 [00:00<00:00, 3.27MB/s]\n",
            "\n",
            "config.json: 100% 592/592 [00:00<00:00, 4.96MB/s]\n",
            "\n",
            "scheduler_config-checkpoint.json: 100% 209/209 [00:00<00:00, 2.43MB/s]\n",
            "\n",
            "special_tokens_map.json: 100% 472/472 [00:00<00:00, 3.36MB/s]\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer_config.json: 100% 806/806 [00:00<00:00, 2.33MB/s]\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 342/342 [00:00<00:00, 3.88MB/s]\n",
            "merges.txt: 525kB [00:00, 27.2MB/s]\n",
            "\n",
            "text_encoder/model.safetensors:   0% 0.00/492M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "vocab.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "config.json: 100% 743/743 [00:00<00:00, 8.54MB/s]\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 551/551 [00:00<00:00, 5.46MB/s]\n",
            "vocab.json: 1.06MB [00:00, 40.4MB/s]\n",
            "\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:   0% 0.00/335M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 0.00/3.44G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:   0% 2.20M/492M [00:00<03:16, 2.49MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:   2% 11.5M/492M [00:01<00:36, 13.3MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:   7% 36.9M/492M [00:01<00:09, 48.6MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  18% 86.9M/492M [00:01<00:03, 122MB/s] \u001b[A\n",
            "text_encoder/model.safetensors:  23% 111M/492M [00:01<00:02, 137MB/s] \u001b[A\n",
            "text_encoder/model.safetensors:  28% 136M/492M [00:01<00:02, 160MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  37% 184M/492M [00:01<00:01, 220MB/s]\u001b[A\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  20% 66.6M/335M [00:01<00:06, 43.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors:  60% 201M/335M [00:01<00:00, 153MB/s]  \u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  45% 221M/492M [00:01<00:01, 218MB/s]\u001b[A\n",
            "\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 260MB/s]\u001b[A\u001b[A\n",
            "vae/diffusion_pytorch_model.safetensors: 100% 335M/335M [00:01<00:00, 184MB/s]\n",
            "\n",
            "text_encoder/model.safetensors:  66% 326M/492M [00:02<00:00, 288MB/s]\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:   0% 1.45M/3.44G [00:02<1:19:46, 718kB/s]\u001b[A\u001b[A\u001b[A\n",
            "text_encoder/model.safetensors:  73% 361M/492M [00:02<00:00, 266MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  79% 390M/492M [00:02<00:00, 239MB/s]\u001b[A\n",
            "text_encoder/model.safetensors:  93% 460M/492M [00:02<00:00, 315MB/s]\u001b[A\n",
            "\n",
            "\n",
            "text_encoder/model.safetensors: 100% 492M/492M [00:02<00:00, 180MB/s]\n",
            "Fetching 14 files:  43% 6/14 [00:03<00:04,  1.92it/s]\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  29% 1.01G/3.44G [00:02<00:04, 597MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  37% 1.28G/3.44G [00:02<00:02, 721MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  44% 1.50G/3.44G [00:03<00:02, 746MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  49% 1.70G/3.44G [00:03<00:02, 774MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  55% 1.90G/3.44G [00:03<00:01, 890MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  61% 2.10G/3.44G [00:03<00:01, 1.00GB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  67% 2.30G/3.44G [00:03<00:01, 1.10GB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  73% 2.50G/3.44G [00:04<00:01, 757MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  79% 2.70G/3.44G [00:04<00:01, 590MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  82% 2.84G/3.44G [00:05<00:01, 443MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  86% 2.97G/3.44G [00:05<00:01, 431MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  88% 3.04G/3.44G [00:05<00:00, 445MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  92% 3.17G/3.44G [00:05<00:00, 493MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors:  94% 3.24G/3.44G [00:06<00:00, 512MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "unet/diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [00:06<00:00, 545MB/s]\n",
            "Fetching 14 files: 100% 14/14 [00:06<00:00,  2.07it/s]\n",
            "Loading pipeline components...:  33% 2/6 [00:01<00:02,  1.87it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading pipeline components...: 100% 6/6 [00:01<00:00,  4.44it/s]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "Model loaded on cuda\n",
            "\n",
            "============================================================\n",
            "Collecting Latents\n",
            "============================================================\n",
            "\n",
            "Collecting safe samples...\n",
            "Safe:  14% 14/100 [00:16<01:30,  1.05s/it]"
          ]
        }
      ],
      "source": [
        "# Collect latents for probe training\n",
        "# A100 optimized: Can handle 300 samples efficiently\n",
        "# Using SD 1.4 (20 steps) - less censored than SD 1.5, better for research\n",
        "# STRICT: Exactly 300 SEXUAL images (sexual category only)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: COLLECTING LATENTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"This will:\")\n",
        "print(\"  - Download Stable Diffusion 1.4 model (~4GB)\")\n",
        "print(\"  - Generate 300 safe prompts (diverse categories)\")\n",
        "print(\"  - Generate EXACTLY 300 SEXUAL unsafe prompts (sexual category only)\")\n",
        "print(\"  - Save latents at each timestep\")\n",
        "print(\"  - Estimated time: 90-120 minutes (A100)\")\n",
        "print(\"=\"*60)\n",
        "print(\"⚠ IMPORTANT: Unsafe prompts are STRICTLY sexual category only\")\n",
        "print(\"   Code will automatically relax criteria if needed to reach 300\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check if CUDA is available\n",
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"⚠ Warning: CUDA not available. This will be very slow!\")\n",
        "    device = \"cpu\"\n",
        "else:\n",
        "    device = \"cuda\"\n",
        "\n",
        "!python scripts/collect_latents.py \\\n",
        "    --num_samples 300 \\\n",
        "    --num_steps 20 \\\n",
        "    --device {device} \\\n",
        "    --model_id CompVis/stable-diffusion-v1-4 \\\n",
        "    --focus_nudity \\\n",
        "    --hard_only \\\n",
        "    --min_inappropriate_pct 60.0 \\\n",
        "    --min_nudity_pct 50.0 \\\n",
        "    --save_images\n",
        "\n",
        "# Verify output and confirm sexual prompt count\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "if latents_dirs:\n",
        "    latest = latents_dirs[-1]\n",
        "    print(f\"\\n✓ Latents collected! Output: {latest}\")\n",
        "\n",
        "    # Count files\n",
        "    latent_files = list(latest.glob('latents/timestep_*.npz'))\n",
        "    print(f\"  Found {len(latent_files)} timestep files\")\n",
        "    \n",
        "    # Verify prompt counts\n",
        "    safe_prompts_file = latest / \"safe_prompts.json\"\n",
        "    unsafe_prompts_file = latest / \"unsafe_prompts.json\"\n",
        "    \n",
        "    if safe_prompts_file.exists():\n",
        "        with open(safe_prompts_file) as f:\n",
        "            safe_prompts = json.load(f)\n",
        "        print(f\"  ✓ Safe prompts: {len(safe_prompts)}\")\n",
        "    \n",
        "    if unsafe_prompts_file.exists():\n",
        "        with open(unsafe_prompts_file) as f:\n",
        "            unsafe_prompts = json.load(f)\n",
        "        \n",
        "        # Count sexual-only prompts\n",
        "        sexual_count = sum(1 for p in unsafe_prompts if \"sexual\" in p.get(\"categories\", \"\").lower())\n",
        "        print(f\"  ✓ Unsafe prompts: {len(unsafe_prompts)}\")\n",
        "        print(f\"  ✓ Sexual category prompts: {sexual_count}\")\n",
        "        \n",
        "        if sexual_count < 300:\n",
        "            print(f\"\\n⚠ Warning: Only {sexual_count} sexual prompts found (requested 300)\")\n",
        "            print(\"   This may be due to:\")\n",
        "            print(\"   - Dataset limitations\")\n",
        "            print(\"   - Strict filtering criteria\")\n",
        "            print(\"   - CLIP token length limits\")\n",
        "        elif sexual_count == 300:\n",
        "            print(f\"\\n✓ Perfect! Exactly {sexual_count} sexual prompts collected\")\n",
        "        else:\n",
        "            print(f\"\\n✓ Found {sexual_count} sexual prompts (target was 300)\")\n",
        "    \n",
        "    # Verify latent data\n",
        "    if latent_files:\n",
        "        sample_file = latent_files[0]\n",
        "        data = np.load(sample_file)\n",
        "        labels = data['y']\n",
        "        safe_count = np.sum(labels == 0)\n",
        "        unsafe_count = np.sum(labels == 1)\n",
        "        print(f\"\\n  Label distribution in latents:\")\n",
        "        print(f\"    Safe (0): {safe_count}\")\n",
        "        print(f\"    Unsafe (1): {unsafe_count}\")\n",
        "        print(f\"    Total: {len(labels)}\")\n",
        "else:\n",
        "    print(\"\\n⚠ Warning: No latents directory found. Check for errors above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1GKFMr2ObW5"
      },
      "source": [
        "## Step 5.25: (Optional) Verify Labels ⭐ NEW - RECOMMENDED\n",
        "\n",
        "**Important:** Before training probes, verify that generated images match their labels!\n",
        "\n",
        "This step uses CLIP to verify that images actually match their prompt-based labels. This is critical because SD 1.4 may generate safe images from unsafe prompts (censorship), leading to poor separability.\n",
        "\n",
        "**Note:** This takes ~10-20 minutes but significantly improves probe accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJO0O4NHObW5"
      },
      "outputs": [],
      "source": [
        "# Verify labels using CLIP-based safety classifier\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VERIFYING LABELS\")\n",
        "print(\"=\"*60)\n",
        "print(\"This step:\")\n",
        "print(\"  - Decodes images from latents\")\n",
        "print(\"  - Uses CLIP to verify labels match images\")\n",
        "print(\"  - Filters out mismatched samples\")\n",
        "print(\"  - Creates cleaned dataset\")\n",
        "print(\"  - Estimated time: 10-20 minutes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "if latents_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    print(f\"\\nUsing latents from: {latest_latents}\")\n",
        "\n",
        "    # Check if already verified\n",
        "    verified_dir = latest_latents.parent / f\"{latest_latents.name}_verified\"\n",
        "    if verified_dir.exists():\n",
        "        print(f\"\\n✓ Verified dataset already exists: {verified_dir}\")\n",
        "        print(\"  Skipping verification. To re-verify, delete this directory first.\")\n",
        "    else:\n",
        "        print(f\"\\nVerifying labels...\")\n",
        "\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        !python scripts/verify_labels.py \\\n",
        "            --latents_dir {latest_latents} \\\n",
        "            --method clip \\\n",
        "            --threshold 0.7 \\\n",
        "            --device {device}\n",
        "\n",
        "        # Check if verification succeeded\n",
        "        if verified_dir.exists():\n",
        "            print(f\"\\n✓ Verification complete! Verified dataset: {verified_dir}\")\n",
        "\n",
        "            # Show statistics\n",
        "            mismatch_file = verified_dir / \"mismatch_report.json\"\n",
        "            if mismatch_file.exists():\n",
        "                import json\n",
        "                with open(mismatch_file) as f:\n",
        "                    mismatches = json.load(f)\n",
        "                print(f\"  Mismatches found: {len(mismatches)}\")\n",
        "\n",
        "                if len(mismatches) > 0:\n",
        "                    print(f\"\\n⚠ Warning: {len(mismatches)} samples had mismatched labels!\")\n",
        "                    print(\"  Review mismatch_report.json for details.\")\n",
        "                    print(\"  Consider:\")\n",
        "                    print(\"    - Using stricter prompt filtering\")\n",
        "                    print(\"    - Using a more explicit model\")\n",
        "                    print(\"    - Manually reviewing generated images\")\n",
        "        else:\n",
        "            print(\"\\n⚠ Warning: Verification may have failed. Check for errors above.\")\n",
        "else:\n",
        "    print(\"⚠ Error: No latents found! Run Step 4 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMMXJ76vObW6"
      },
      "source": [
        "## Step 5.5: (Optional) Measure Empirical Layer Sensitivity ⭐ NEW\n",
        "\n",
        "**Recommended for best results:** Measure FID and SSR empirically instead of using heuristics.\n",
        "\n",
        "This step runs small steering experiments to measure:\n",
        "- **Quality preservation**: FID between steered and unsteered images (Heusel et al., 2017)\n",
        "- **Steering effectiveness**: SSR improvement from steering\n",
        "\n",
        "**Note:** This takes additional time (~30-60 min) but provides more accurate sensitivity scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3RPCWMDObW6"
      },
      "outputs": [],
      "source": [
        "# Measure empirical layer sensitivity (FID and SSR)\n",
        "# This improves the quality of layer sensitivity analysis\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MEASURING EMPIRICAL LAYER SENSITIVITY\")\n",
        "print(\"=\"*60)\n",
        "print(\"This step:\")\n",
        "print(\"  - Runs small steering experiments at each timestep\")\n",
        "print(\"  - Measures FID (quality preservation)\")\n",
        "print(\"  - Measures SSR (steering effectiveness)\")\n",
        "print(\"  - Estimated time: 30-60 minutes\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "if latents_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    print(f\"\\nUsing latents from: {latest_latents}\")\n",
        "\n",
        "    # Use probe from Step 5 if available\n",
        "    probe_path = None\n",
        "    if probe_dirs:\n",
        "        latest_probe = probe_dirs[-1] / 'pytorch'\n",
        "        if latest_probe.exists():\n",
        "            probe_path = str(latest_probe)\n",
        "            print(f\"Using probe: {probe_path}\")\n",
        "        else:\n",
        "            print(\"⚠ Warning: Probe directory exists but pytorch/ subdirectory not found\")\n",
        "    else:\n",
        "        print(\"⚠ Warning: No probes found. Running without probe (will use random steering)\")\n",
        "\n",
        "    # Check if already measured\n",
        "    quality_file = latest_latents / \"quality_measurements.json\"\n",
        "    effectiveness_file = latest_latents / \"effectiveness_measurements.json\"\n",
        "\n",
        "    if quality_file.exists() and effectiveness_file.exists():\n",
        "        print(\"\\n✓ Measurements already exist! Skipping measurement.\")\n",
        "        print(f\"  Quality: {quality_file}\")\n",
        "        print(f\"  Effectiveness: {effectiveness_file}\")\n",
        "        print(\"\\nTo re-measure, delete these files first.\")\n",
        "    else:\n",
        "        print(f\"\\nMeasuring empirical sensitivity...\")\n",
        "        print(\"This may take 30-60 minutes...\")\n",
        "\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        if probe_path:\n",
        "            !python scripts/measure_layer_sensitivity.py \\\n",
        "                --latents_dir {latest_latents} \\\n",
        "                --num_samples 20 \\\n",
        "                --device {device} \\\n",
        "                --probe_path {probe_path}\n",
        "        else:\n",
        "            !python scripts/measure_layer_sensitivity.py \\\n",
        "                --latents_dir {latest_latents} \\\n",
        "                --num_samples 20 \\\n",
        "                --device {device}\n",
        "\n",
        "        # Verify measurements were created\n",
        "        quality_file = latest_latents / \"quality_measurements.json\"\n",
        "        effectiveness_file = latest_latents / \"effectiveness_measurements.json\"\n",
        "\n",
        "        if quality_file.exists():\n",
        "            print(f\"\\n✓ Quality measurements saved: {quality_file}\")\n",
        "        else:\n",
        "            print(f\"\\n⚠ Warning: Quality measurements not found\")\n",
        "\n",
        "        if effectiveness_file.exists():\n",
        "            print(f\"✓ Effectiveness measurements saved: {effectiveness_file}\")\n",
        "        else:\n",
        "            print(f\"⚠ Warning: Effectiveness measurements not found\")\n",
        "\n",
        "        if quality_file.exists() and effectiveness_file.exists():\n",
        "            print(\"\\n✓ Measurements complete! Now re-run Step 5 to use them.\")\n",
        "        else:\n",
        "            print(\"\\n⚠ Some measurements missing. Check for errors above.\")\n",
        "else:\n",
        "    print(\"⚠ Error: No latents found! Run Step 4 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofDSxAc-ObW6"
      },
      "source": [
        "## Step 5: Phase 1 - Train Probes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd2oQueRObW6"
      },
      "outputs": [],
      "source": [
        "# Train linear probes\n",
        "# Find the latest latents directory\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: TRAINING LINEAR PROBES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "if latents_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    print(f\"Using latents from: {latest_latents}\")\n",
        "\n",
        "    # Check if empirical measurements exist\n",
        "    use_empirical = False\n",
        "    quality_file = latest_latents / \"quality_measurements.json\"\n",
        "    effectiveness_file = latest_latents / \"effectiveness_measurements.json\"\n",
        "\n",
        "    if quality_file.exists() and effectiveness_file.exists():\n",
        "        print(\"✓ Found empirical measurements! Using them for better accuracy.\")\n",
        "        use_empirical = True\n",
        "    else:\n",
        "        print(\"Using improved heuristics (faster). For better accuracy, run Step 5.5 first.\")\n",
        "\n",
        "    # Train probes\n",
        "    if use_empirical:\n",
        "        print(\"\\nTraining with empirical measurements...\")\n",
        "        !python scripts/train_probes.py --latents_dir {latest_latents} --use_empirical\n",
        "    else:\n",
        "        print(\"\\nTraining with heuristics...\")\n",
        "        !python scripts/train_probes.py --latents_dir {latest_latents}\n",
        "\n",
        "    # Print probe results summary\n",
        "    probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "    if probe_dirs:\n",
        "        latest_probe = probe_dirs[-1]\n",
        "        metrics_file = latest_probe / 'probe_metrics.json'\n",
        "        sensitivity_file = latest_probe / 'sensitivity_scores.json'\n",
        "\n",
        "        if metrics_file.exists():\n",
        "            with open(metrics_file) as f:\n",
        "                metrics = json.load(f)\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"PROBE ACCURACY SUMMARY\")\n",
        "            print(\"=\"*60)\n",
        "            best_acc = 0\n",
        "            best_t = None\n",
        "            for t in sorted(metrics.keys(), key=int):\n",
        "                acc = metrics[t]['test_acc']\n",
        "                print(f\"Timestep {t:2d}: {acc:.3f} ({acc*100:5.1f}%)\")\n",
        "                if acc > best_acc:\n",
        "                    best_acc = acc\n",
        "                    best_t = t\n",
        "\n",
        "            print(f\"\\n✓ Best accuracy: {best_acc:.3f} at timestep {best_t}\")\n",
        "\n",
        "            # Check sensitivity scores\n",
        "            if sensitivity_file.exists():\n",
        "                with open(sensitivity_file) as f:\n",
        "                    sens_data = json.load(f)\n",
        "\n",
        "                if 'optimal_window' in sens_data:\n",
        "                    window = sens_data['optimal_window']\n",
        "                    print(f\"\\n✓ Recommended intervention window: steps {window.get('start', '?')} to {window.get('end', '?')}\")\n",
        "                    if 'top_timesteps' in window:\n",
        "                        print(f\"  Top timesteps: {window['top_timesteps']}\")\n",
        "        else:\n",
        "            print(\"⚠ Warning: probe_metrics.json not found\")\n",
        "    else:\n",
        "        print(\"⚠ Warning: No probe directories created. Check for errors above.\")\n",
        "else:\n",
        "    print(\"⚠ Error: No latents found! Run Step 4 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ms6q28sObW7"
      },
      "source": [
        "## Step 6: Visualize Generated Images & Verify Probe Accuracy ⭐ NEW\n",
        "\n",
        "**Important:** Before training PPO, verify that the generated images match their labels!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u4oC4mXObW7"
      },
      "outputs": [],
      "source": [
        "# Generate images from collected latents to verify what was actually generated\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GENERATING IMAGES FROM LATENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "if latents_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    print(f\"Using latents from: {latest_latents}\")\n",
        "\n",
        "    # Check if images already exist\n",
        "    viewer_path = latest_latents / \"images_t20/viewer.html\"\n",
        "    if viewer_path.exists():\n",
        "        print(\"\\n✓ Images already generated! Skipping...\")\n",
        "        print(f\"  Viewer: {viewer_path}\")\n",
        "    else:\n",
        "        print(\"\\nGenerating images from final timestep (t=20)...\")\n",
        "        print(\"This may take 5-10 minutes...\")\n",
        "\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        !python scripts/generate_images_from_latents.py \\\n",
        "            --latents_dir {latest_latents} \\\n",
        "            --timestep 20 \\\n",
        "            --num_samples 50 \\\n",
        "            --device {device}\n",
        "\n",
        "        # Check if HTML viewer was created\n",
        "        viewer_path = latest_latents / \"images_t20/viewer.html\"\n",
        "        if viewer_path.exists():\n",
        "            print(f\"\\n✓ Images generated! Viewer: {viewer_path}\")\n",
        "        else:\n",
        "            print(\"\\n⚠ Warning: HTML viewer not found. Check for errors above.\")\n",
        "\n",
        "    # Show how to view\n",
        "    if viewer_path.exists():\n",
        "        print(\"\\nTo view images in Colab, run the next cell!\")\n",
        "else:\n",
        "    print(\"⚠ Error: No latents found! Run Step 4 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDyebLaBObW7"
      },
      "source": [
        "### View Images in Colab\n",
        "\n",
        "Display the HTML viewer directly in the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvbGb0nLObW7"
      },
      "outputs": [],
      "source": [
        "# Display HTML viewer in Colab\n",
        "from IPython.display import HTML, display\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=os.path.getmtime)\n",
        "if latents_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    viewer_path = latest_latents / \"images_t20/viewer.html\"\n",
        "\n",
        "    if viewer_path.exists():\n",
        "        with open(viewer_path, 'r', encoding='utf-8') as f:\n",
        "            html_content = f.read()\n",
        "        display(HTML(html_content))\n",
        "    else:\n",
        "        print(\"Viewer not found. Run the previous cell first.\")\n",
        "else:\n",
        "    print(\"No latents found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZakwWBYLObW7"
      },
      "source": [
        "### Visualize Probe Results\n",
        "\n",
        "See which images are correctly/incorrectly classified by the probe:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzUaCJwpObW7"
      },
      "outputs": [],
      "source": [
        "# Visualize probe predictions on images\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VISUALIZING PROBE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "latents_dirs = sorted(Path('data/latents').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "if latents_dirs and probe_dirs:\n",
        "    latest_latents = latents_dirs[-1]\n",
        "    latest_probe = probe_dirs[-1]\n",
        "    probe_pytorch = latest_probe / 'pytorch'\n",
        "\n",
        "    # Find best timestep from sensitivity analysis\n",
        "    best_timestep = 4  # Default\n",
        "    sensitivity_file = latest_probe / 'sensitivity_scores.json'\n",
        "\n",
        "    if sensitivity_file.exists():\n",
        "        with open(sensitivity_file) as f:\n",
        "            sens_data = json.load(f)\n",
        "\n",
        "        # Find timestep with highest score\n",
        "        best_score = -1\n",
        "        for t_str, data in sens_data.items():\n",
        "            if t_str == \"optimal_window\":\n",
        "                continue\n",
        "            if isinstance(data, dict) and 'score' in data:\n",
        "                score = data['score']\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_timestep = int(t_str)\n",
        "\n",
        "        print(f\"Using best timestep from sensitivity analysis: t={best_timestep} (score={best_score:.3f})\")\n",
        "    else:\n",
        "        print(f\"Using default timestep: t={best_timestep}\")\n",
        "        print(\"  (Run Step 5 to get sensitivity analysis)\")\n",
        "\n",
        "    if not probe_pytorch.exists():\n",
        "        print(f\"⚠ Error: Probe directory not found: {probe_pytorch}\")\n",
        "    else:\n",
        "        print(f\"\\nVisualizing probe results:\")\n",
        "        print(f\"  Latents: {latest_latents}\")\n",
        "        print(f\"  Probe: {probe_pytorch}\")\n",
        "        print(f\"  Timestep: {best_timestep}\")\n",
        "\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        !python scripts/visualize_probe_results.py \\\n",
        "            --latents_dir {latest_latents} \\\n",
        "            --probe_dir {probe_pytorch} \\\n",
        "            --timestep {best_timestep} \\\n",
        "            --num_samples 30 \\\n",
        "            --device {device}\n",
        "\n",
        "        # Display visualization\n",
        "        viz_path = Path('outputs/visualizations') / f'probe_visualization_t{best_timestep:02d}.png'\n",
        "        if viz_path.exists():\n",
        "            from IPython.display import Image, display\n",
        "            print(f\"\\n✓ Visualization:\")\n",
        "            display(Image(str(viz_path)))\n",
        "            print(f\"  Saved to: {viz_path}\")\n",
        "        else:\n",
        "            print(\"\\n⚠ Warning: Visualization not found. Check for errors above.\")\n",
        "else:\n",
        "    if not latents_dirs:\n",
        "        print(\"⚠ Error: No latents found! Run Step 4 first.\")\n",
        "    if not probe_dirs:\n",
        "        print(\"⚠ Error: No probes found! Run Step 5 first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQuIrHrIObW8"
      },
      "source": [
        "## Step 7: Phase 2 - Train PPO Policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKENhDdBObW8"
      },
      "outputs": [],
      "source": [
        "# Train PPO policy with Colab-optimized config\n",
        "# Auto-detects GPU type (A100 vs T4) and uses appropriate config\n",
        "# The config uses probe_path: \"auto\" to automatically find the latest probe\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 2: TRAINING PPO POLICY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Verify prerequisites\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Detect GPU type and select appropriate config\n",
        "gpu_name = \"\"\n",
        "vram_gb = 0\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU detected: {gpu_name}\")\n",
        "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠ Warning: No GPU detected! Training will be very slow.\")\n",
        "\n",
        "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "# Select config based on GPU\n",
        "if \"A100\" in gpu_name or (torch.cuda.is_available() and vram_gb >= 35):\n",
        "    # A100 detected - use A100 optimized config\n",
        "    print(\"\\n✓ A100 GPU detected! Using A100-optimized config\")\n",
        "    print(\"  Available configs:\")\n",
        "    print(\"    - colab_a100_fast.yaml (1-2 hours, quick experiments)\")\n",
        "    print(\"    - colab_a100_optimized.yaml (2-3 hours, recommended)\")\n",
        "    print(\"    - colab_a100_best.yaml (3-4 hours, maximum quality)\")\n",
        "\n",
        "    # Default to optimized, but user can change\n",
        "    config_file = Path('configs/colab_a100_optimized.yaml')\n",
        "\n",
        "    # Check if user wants fast or best mode (can be changed)\n",
        "    use_fast = False  # Set to True for faster training\n",
        "    use_best = False  # Set to True for best quality\n",
        "\n",
        "    if use_fast:\n",
        "        config_file = Path('configs/colab_a100_fast.yaml')\n",
        "        print(\"  → Using FAST config (1-2 hours)\")\n",
        "    elif use_best:\n",
        "        config_file = Path('configs/colab_a100_best.yaml')\n",
        "        print(\"  → Using BEST config (3-4 hours, maximum quality)\")\n",
        "    else:\n",
        "        print(\"  → Using OPTIMIZED config (2-3 hours, recommended)\")\n",
        "\n",
        "    # Fallback chain for A100\n",
        "    if not config_file.exists():\n",
        "        print(f\"⚠ Config not found: {config_file}\")\n",
        "        if Path('configs/colab_a100_fast.yaml').exists():\n",
        "            config_file = Path('configs/colab_a100_fast.yaml')\n",
        "            print(\"  Falling back to colab_a100_fast.yaml...\")\n",
        "        elif Path('configs/colab_optimized.yaml').exists():\n",
        "            config_file = Path('configs/colab_optimized.yaml')\n",
        "            print(\"  Falling back to colab_optimized.yaml...\")\n",
        "else:\n",
        "    # T4 or other GPU - use T4 config\n",
        "    print(\"\\n✓ T4 or other GPU detected. Using T4-optimized config\")\n",
        "    config_file = Path('configs/colab_fast_20steps.yaml')\n",
        "\n",
        "    # Fallback to original config if fast config doesn't exist\n",
        "    if not config_file.exists():\n",
        "        print(f\"⚠ Fast config not found: {config_file}\")\n",
        "        print(\"  Falling back to colab_optimized.yaml...\")\n",
        "        config_file = Path('configs/colab_optimized.yaml')\n",
        "\n",
        "if not probe_dirs:\n",
        "    print(\"⚠ Error: No probes found! Run Step 5 first.\")\n",
        "elif not config_file.exists():\n",
        "    print(f\"⚠ Error: Config file not found: {config_file}\")\n",
        "else:\n",
        "    latest_probe = probe_dirs[-1]\n",
        "    print(f\"Using probe: {latest_probe}\")\n",
        "    print(f\"Config: {config_file}\")\n",
        "\n",
        "    # Print training settings based on config\n",
        "    if \"a100\" in str(config_file).lower():\n",
        "        if \"fast\" in str(config_file).lower():\n",
        "            print(\"\\nTraining settings (A100 FAST MODE - 1-2 hours):\")\n",
        "            print(\"  - Total timesteps: 50,000\")\n",
        "            print(\"  - Batch size: 128 (A100 optimized)\")\n",
        "            print(\"  - Rollout size: 256\")\n",
        "            print(\"  - Epochs: 4\")\n",
        "            print(\"  - Estimated time: 1-2 hours\")\n",
        "        elif \"best\" in str(config_file).lower():\n",
        "            print(\"\\nTraining settings (A100 BEST MODE - 3-4 hours):\")\n",
        "            print(\"  - Total timesteps: 200,000\")\n",
        "            print(\"  - Batch size: 128 (A100 optimized)\")\n",
        "            print(\"  - Rollout size: 512\")\n",
        "            print(\"  - Epochs: 8\")\n",
        "            print(\"  - Policy: [1024, 512, 256] (largest)\")\n",
        "            print(\"  - Estimated time: 3-4 hours\")\n",
        "        else:\n",
        "            print(\"\\nTraining settings (A100 OPTIMIZED MODE - 2-3 hours):\")\n",
        "            print(\"  - Total timesteps: 100,000\")\n",
        "            print(\"  - Batch size: 128 (A100 optimized)\")\n",
        "            print(\"  - Rollout size: 256\")\n",
        "            print(\"  - Epochs: 6\")\n",
        "            print(\"  - Policy: [1024, 512, 256]\")\n",
        "            print(\"  - Estimated time: 2-3 hours\")\n",
        "    else:\n",
        "        print(\"\\nTraining settings (T4 FAST MODE - 2-3 hours):\")\n",
        "        print(\"  - Total timesteps: 50,000\")\n",
        "        print(\"  - Batch size: 32 (T4 optimized)\")\n",
        "        print(\"  - Epochs: 4\")\n",
        "        print(\"  - Estimated time: 2-3 hours\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"⚠ Warning: No GPU detected! Training will be very slow.\")\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    !python scripts/train_ppo.py --config {config_file}\n",
        "\n",
        "    # Check if training completed\n",
        "    ppo_dirs = sorted(Path('outputs/ppo').glob('aether_ppo_*'), key=lambda p: p.stat().st_mtime)\n",
        "    if ppo_dirs:\n",
        "        latest_run = ppo_dirs[-1]\n",
        "        policy_file = latest_run / 'final_policy.pt'\n",
        "        if policy_file.exists():\n",
        "            print(f\"\\n✓ Training complete! Policy saved: {policy_file}\")\n",
        "        else:\n",
        "            print(f\"\\n⚠ Warning: final_policy.pt not found. Check for errors above.\")\n",
        "            print(f\"  Run directory: {latest_run}\")\n",
        "    else:\n",
        "        print(\"\\n⚠ Warning: No training output found. Check for errors above.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDZUY13OObW8"
      },
      "source": [
        "## Step 8: Phase 3 - Evaluate Policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDBZMBwTObW8"
      },
      "source": [
        "### Option A: Quick Evaluation\n",
        "\n",
        "Evaluate the trained policy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zao1CwkSObW8"
      },
      "outputs": [],
      "source": [
        "# Evaluate trained policy\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 3: EVALUATING POLICY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find latest policy and probe\n",
        "ppo_dirs = sorted(Path('outputs/ppo').glob('aether_ppo_*'), key=lambda p: p.stat().st_mtime)\n",
        "probe_dirs = sorted(Path('checkpoints/probes').glob('run_*'), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "if not ppo_dirs:\n",
        "    print(\"⚠ Error: No training runs found! Run Step 7 first.\")\n",
        "elif not probe_dirs:\n",
        "    print(\"⚠ Error: No probe directories found! Run Step 5 first.\")\n",
        "else:\n",
        "    latest_policy = ppo_dirs[-1] / 'final_policy.pt'\n",
        "    latest_probe = probe_dirs[-1] / 'pytorch'\n",
        "\n",
        "    print(f\"Policy: {latest_policy}\")\n",
        "    print(f\"Probe: {latest_probe}\")\n",
        "\n",
        "    if not latest_policy.exists():\n",
        "        print(f\"⚠ Error: Policy file not found: {latest_policy}\")\n",
        "        print(f\"  Available files in {ppo_dirs[-1]}:\")\n",
        "        for f in ppo_dirs[-1].glob('*.pt'):\n",
        "            print(f\"    - {f.name}\")\n",
        "    elif not latest_probe.exists():\n",
        "        print(f\"⚠ Error: Probe directory not found: {latest_probe}\")\n",
        "    else:\n",
        "        print(\"\\nEvaluation metrics:\")\n",
        "        print(\"  - SSR (Safety Success Rate): Higher is better\")\n",
        "        print(\"  - FPR (False Positive Rate): Lower is better\")\n",
        "        print(\"  - LPIPS (Perceptual Distance): Lower is better\")\n",
        "        print(\"  - Transport Cost: Lower is better\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        import torch\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        !python scripts/evaluate_ppo.py \\\n",
        "            --policy_path {latest_policy} \\\n",
        "            --probe_path {latest_probe} \\\n",
        "            --num_samples 50 \\\n",
        "            --device {device}\n",
        "\n",
        "        # Check for evaluation results\n",
        "        eval_dirs = sorted(Path('outputs/evaluation').glob('eval_*'), key=lambda p: p.stat().st_mtime)\n",
        "        if eval_dirs:\n",
        "            latest_eval = eval_dirs[-1]\n",
        "            print(f\"\\n✓ Evaluation complete! Results: {latest_eval}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsumpVWGObW8"
      },
      "source": [
        "### Option B: Run Multiple Experiments (Compare Hyperparameters)\n",
        "\n",
        "Run different hyperparameter configurations to compare results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiihnmxVObW8"
      },
      "outputs": [],
      "source": [
        "# Run all experiments (takes ~9-12 hours total)\n",
        "# Each experiment: ~1.5-2 hours, 100K timesteps\n",
        "!python scripts/run_experiments.py --all\n",
        "\n",
        "# Or run specific experiments:\n",
        "# !python scripts/run_experiments.py --experiments exp1 exp2 exp3\n",
        "\n",
        "# Experiments:\n",
        "# exp1: Low lambda (0.3) - aggressive safety\n",
        "# exp2: Medium lambda (0.5) - balanced\n",
        "# exp3: High lambda (0.8) - efficient actions\n",
        "# exp4: Fast learning rate (3e-4)\n",
        "# exp5: More epochs (12)\n",
        "# exp6: Smaller policy (256,128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJRTFfqEObW8"
      },
      "outputs": [],
      "source": [
        "### Option C: Run Single Experiment Manually\n",
        "\n",
        "Run a specific experiment configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wr39IqObW9"
      },
      "outputs": [],
      "source": [
        "# Example: Run experiment 1 (low lambda)\n",
        "!python scripts/train_ppo.py --config configs/colab_experiment_1_low_lambda.yaml\n",
        "\n",
        "# Other experiments:\n",
        "# !python scripts/train_ppo.py --config configs/colab_experiment_2_medium_lambda.yaml\n",
        "# !python scripts/train_ppo.py --config configs/colab_experiment_3_high_lambda.yaml\n",
        "# !python scripts/train_ppo.py --config configs/colab_experiment_4_fast_learning.yaml\n",
        "# !python scripts/train_ppo.py --config configs/colab_experiment_5_more_epochs.yaml\n",
        "# !python scripts/train_ppo.py --config configs/colab_experiment_6_smaller_policy.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txK3njJ-ObW9"
      },
      "source": [
        "## Step 9: Save Results to Google Drive\n",
        "\n",
        "Mount your Google Drive and save results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYHGJ5jiObW9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"SAVING RESULTS TO GOOGLE DRIVE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Mount Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy results to Drive\n",
        "drive_path = Path('/content/drive/MyDrive/project-aether-results')\n",
        "drive_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\nCopying results to: {drive_path}\")\n",
        "\n",
        "# Copy outputs\n",
        "if Path('outputs').exists():\n",
        "    print(\"  Copying outputs/...\")\n",
        "    !cp -r outputs {drive_path}/\n",
        "    print(\"    ✓ outputs/\")\n",
        "\n",
        "# Copy checkpoints\n",
        "if Path('checkpoints').exists():\n",
        "    print(\"  Copying checkpoints/...\")\n",
        "    !cp -r checkpoints {drive_path}/\n",
        "    print(\"    ✓ checkpoints/\")\n",
        "\n",
        "# Copy latents\n",
        "if Path('data/latents').exists():\n",
        "    print(\"  Copying data/latents/...\")\n",
        "    !cp -r data/latents {drive_path}/\n",
        "    print(\"    ✓ data/latents/\")\n",
        "\n",
        "# Also copy visualization results if they exist\n",
        "viz_path = Path('outputs/visualizations')\n",
        "if viz_path.exists():\n",
        "    print(\"  Copying visualizations/...\")\n",
        "    !cp -r {viz_path} {drive_path}/outputs/\n",
        "    print(\"    ✓ visualizations/\")\n",
        "\n",
        "print(f\"\\n✓ Results saved to: {drive_path}\")\n",
        "print(f\"\\nSaved directories:\")\n",
        "print(f\"  - Training outputs: {drive_path}/outputs/\")\n",
        "print(f\"  - Probes: {drive_path}/checkpoints/probes/\")\n",
        "print(f\"  - Latents and images: {drive_path}/data/latents/\")\n",
        "if viz_path.exists():\n",
        "    print(f\"  - Visualizations: {drive_path}/outputs/visualizations/\")\n",
        "\n",
        "# Show size\n",
        "import subprocess\n",
        "result = subprocess.run(['du', '-sh', str(drive_path)], capture_output=True, text=True)\n",
        "if result.returncode == 0:\n",
        "    size = result.stdout.split()[0]\n",
        "    print(f\"\\nTotal size: {size}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
